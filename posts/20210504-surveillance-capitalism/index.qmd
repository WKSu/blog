---
toc: true
layout: post
description: And why I started a blog to share my life. 
categories: [technology, surveillance capitalism]
title: My opposition to social media
image: "https://cdn.pixabay.com/photo/2018/03/02/03/44/unordered-3192273_1280.png"
date: "2021-05-04"
---

> There are many reasons to write a blog but I will detail mine. Writing has always been a sore spot of mine. Simply put, I am not very good at writing, and I struggle with the process. It could always be a little bit better. But I decided this negative association doesn't have to be there forever in my life. There are many positives to gain from constructing loose thoughts into concise texts. Next to it being an outlet for creativity I also want to work on how to communicate my thoughts better. The Sense of Style by Steven Pinker has given me a lot of practical tips and I wish to practice them in the near future.

> The other reason is my opinion on our contemporary digital landscape. The digital revolution led to the rise of social media which allows us to connect with others. I do believe you can use social media for good, connecting with people and as a information hub. Yet, I am resistant towards the corporations behind the technology. Often they focus on our narcissistic tendencies to compare ourselves to others, reinforce our competitive mind, or non-organically influencing our thoughts harming true connection. I find it difficult to break the systematic tendencies of these companies, which is why I prefer the clean, long-form content without [deceptive patterns](https://edpb.europa.eu/system/files/2022-03/edpb_03-2022_guidelines_on_dark_patterns_in_social_media_platform_interfaces_en.pdf). 

> My skepticisms has only grown stronger the last few years. This following essay details the harmful impact of recommendation systems, which is written with a friend of mine. It gives an understanding to the addictive tendencies behind current day social media. To summarize, the incentives of the technology companies are wrongly aligned with what is good for me and society. Their aim is to get your attention span and keep it, using whatever means possible in order to profit off your data. In this system, aptly named [surveillance capitalism](https://www.wikiwand.com/en/Surveillance_capitalism).

> Ironically, a year after Netflix aired [The Social Dilemma](https://www.wikiwand.com/en/The_Social_Dilemma) featuring similar arguments. Another must read on this topic that shaped my reluctance towards sharing your data without owning them is the book [Je hebt wél iets te verbergen](https://decorrespondent.nl/nietsteverbergen). A deep dive in what data is taken, how they can be used, and why it is so important to protect our right to privacy. Giving real kafkaesque situations with the misuse of data by private and public organisations alike. [This concise blog post](https://moxie.org/2013/06/12/we-should-all-have-something-to-hide.html) by Moxie Marlinspike (founder of Signal) reinforces why it is so important to be concerned about your digital footprint. 

<!-- > This website is published with the help of [Quarto](https://quarto.org/) and Github Pages. Cloudflare helps deal with the traffic, while the domain is hosted on freenom.  -->

# YouTube’s recommender algorithm; the friend we did not ask for
_Unregulated recommender systems have a significant influence on your opinion and world view. How do we regulate them?_
Written by Brennen Bouwmeester and Kevin Su 

 ![](https://cdn.pixabay.com/photo/2018/03/02/03/44/unordered-3192273_1280.png){fig-align="center" width=480}

How one’s mind can be influenced by many factors. Illustration taken from [Pixabay](https://pixabay.com/nl/illustrations/ongeordende-chaos-3192273/)

## Introduction
The Master of Science Engineering and Policy Analysis (EPA) has one word in the vocabulary of every student: EPA-life. A term for when your mind can only think of the unsolvable, complex, international problems we face. Working tirelessly every day my mind was keen on distraction. There was no lack of options either: Netflix, Facebook, and YouTube all offered much-needed relaxation. Ultimately, YouTube was the more convincing offer. I would work in the Hague until 6 p.m., take the tram home and then, together with over 30 million others worldwide, watch YouTube videos to relax. After searching for “chess” and observing Magnus Carlsen pull off a wonderful win using the Sicilian defense, something peculiar happened.

A video played, showing cheaters in the world of chess. After clicking back to a memorable game between Ajax and Real Madrid, YouTube took me towards videos on bribery. Outlandish claims became more present the further I went. The pattern continued as I searched for more topics, always leading me to more extremist videos on the topic I had initially searched for. Documentaries on the second world war led to holocaust denial videos, speeches by American presidents led to smear campaigns. YouTube’s autoplay function and the recommendation system drew my curiosity.

## The Influence of YouTube's Recommendation Algorithm
It used to be that my friends would advise me what to watch, now YouTube seems to have replaced them. In return we do not need to spend any effort finding the next video that will get our minds of things. Using smart personalized suggestions, based on the enormous amount of data Alphabet Inc (owner of YouTube and Google services) owns, they know what makes you click. The recommender system keeps people interested and glued to the screen. The system recommends around 70% of the videos watched by users; the influence of YouTube is unavoidable. But why does this algorithm drive us to extremist content, with no intention from our side?

To answer that question, we need to inspect YouTube’s business model. As a platform, it brings together content creators and content viewers. YouTube makes money by brokering fitting advertisements to users. To optimize this process, their goal is to maximize the time spent on YouTube. This has not gone unnoticed by Reed Hastings, CEO of Netflix. During the earnings call he said: “Netflix’s biggest competitors are YouTube, Facebook, and sleep”. The attention economy views time as a scarce resource, if you spend time on one thing, it means a loss of time for something else. In the past, the market focused on getting money, nowadays, this fight is about time spent. More time on a platform means more advertisements, thus increasing YouTube’s share of the pie. This effect also takes place on YouTube itself, as they pay content creators proportional to the number of views they get on their videos, which again incentivizes to maximize the time spent by the users.

## Human Characteristics Exploited by YouTube
To maximize this time, YouTube makes use of different human characteristics. First, people spend time on content out of the ordinary. Think of the number of movies or books about a regular person going to his regular job every day: The Godfather does not manage a supermarket, Batman and the Joker do not run a restaurant, and Frodo does not bring back a book to the library. Deep love dramas, heavy war stories, and extreme conspiracy theories attract people. This is no different when watching a YouTube video on the couch, where clickbait titles and extremist videos are just one click away.

What makes things even worse is the human nature of curiosity. YouTube speaks to our core instincts by showing the tip of the iceberg, often in the shape of an interesting-looking clickbait title. Combined with the human skill of habituation YouTube’s influence increases. As with any addiction, for the first time only a small portion is enough, but as time passes, the portion has to increase more and more to keep us satisfied. Zeynep Tüfekçi, writer and academic on the social impact of technology, explained YouTube’s algorithm with the following example: a restaurant that continuously loads our plates up with sugary and fatty foods, in turn we keep wanting more. As time passes, the videos get more and more extremist.

Another human characteristic is the sheep-like behaviour of watching videos others have watched. This effect means the most extremist videos get watched and recommended repeatedly at an increasing speed. If all this was not enough, most of the views on extremist videos come from a small group of heavy users. This is like recommending a diet based on regular customers of a fast-food chain. Guillaume Chaslot, a former employee of YouTube, claimed the effect of sheep-like behaviour and heavy users increase extremists videos.

So part of the large availability of extremist content is inherent to the business model of YouTube. However, a second reason for the presence of extremist videos is the existence of ideological attackers. Ironically enough, YouTuber “Smarter every day”, showed a series of computer made videos, that somehow pierced through the defence mechanisms of the video platform. In combination with today’s ability to buy fake followers and subscribers to watch your videos, it is relatively easy to get your computer made extremist videos on the autoplay lists of over 30 million people every day.

Not all parts of the recommendation are troublesome. Every day, people get recommended videos they love: new music videos, interesting chess games, highlights of a great football match. It is convenient and saves a lot of time, which is scarce in today’s society. However, are we aware of the price we pay for convenience? It is unclear how YouTube’s black box of recommendation works, despite having a huge impact on society. It is about time that the 30 million extremist content addicted people should rehabilitate.

## YouTube's Role and Responsibility
Is YouTube responsible for our human nature to keep digging for more? Is it the viewers responsibility to understand that they are being influenced and prevent the effect? YouTube claims they are only a facilitating medium showing us the videos we want to see. There is a certain truth to that, of course we do not have to watch the recommended videos. We still have a choice. However, how informed is this choice? These corporations are driven by money to keep our attention on a platform. To do so, they have removed a big barrier: deciding what to watch. Imagine if you had to think yourself what to watch every night; indeed, you would be inclined to move on to something more convenient. In essence, corporations nowadays have fulfilled this decision of what we spent our time on for us. Though, looking at it that way, it is similar to voting for the national elections where someone has already filled in a choice for you, without explaining how they arrived at this conclusion. You could still erase that choice but convenience dictates that the majority will leave it there. Even though YouTube has “only” a facilitating role, their influence is significant to say the least. YouTube possesses the power to decide what users watch.

Users however, cannot get a grasp on what YouTube is actually doing to recommend videos. Since the introduction of their personalized recommendations with a deep neural network they have tweaked and changed the algorithm behind closed curtains. There is no way to know for certain what they actually did, as they will never reveal it voluntarily. It is in their best interest to find the ‘best’ algorithm to keep viewers attracted to their platform, in order to outrun competition. However, when the curtain gets lifted, it does not look good. In an interview with Bloomberg, it is stated that many engineers at YouTube claimed that the leaders were not interested in removing extremist videos or conspiracies. Yonatan Zunger, privacy engineer at the time of the introduction of the neural network algorithm, mentioned that he had a solution in place for videos close to breaching the community guidelines, allow them on the platform but do not recommend them, which was promptly rejected. Leadership only had one goal, which was getting more engagement from users: don’t rock the boat.

The adjustments that YouTube did reveal under the pressure of the media are a disappointment.

The survivors of the Parkland shooting event of 2018, where tragically seventeen people died, were met with online harassment. Conspiracy theories claiming that the event was not real were trending on social media, including YouTube. The YouTube recommender system kept promoting these conspiracy videos to the top. As these videos were often the first videos users would see when searching for the shooting, it shaped the minds of many people.

The danger lies in the ease of how a wrong message gets spread without the proper authority or sources to back the claims in this message. To combat this example of actively sharing wrong information, YouTube publicly announced they would add information from Wikimedia to inform people watching conspiracy videos. Worryingly, this only changes the narrative of YouTube’s responsibility. Instead of making sure that these harmful videos do not have a (active) place on their platform, they shift the burden to Wikipedia to make sure accurate information is placed. The head of Wikipedia’s public relations department, Katherine Maher, had some reservations. While Wikipedia is quite accurate, it relies on crowdsourcing and contributions of volunteers. According to Maher people need to be critical and check the sources which they simply ‘can’t do in a simple search result’, like in the case of the Parkland shooting.

Not only Wikipedia articles will be included, but also  ‘authoritative’ news will be included. YouTube then holds the power which news source will be published and deemed the authoritative source on the matter. The foundation of this algorithm also has significant flaws. The Washington Post revealed that the top news link for final election results led to a news site with false numbers. Attention to media bias has risen with polarized elections throughout the world. YouTube therefore now holds even more power by deciding which news articles to promote.
The dangers of the recommender system do not only lie in extremist or conspiracy videos. Through recommendations a pedophile network was set up not long ago. These pedophiles would look at videos of children showing genitals, while the comments were full of time-stamps of these events. The algorithm did exactly what it was set out to do: find videos that kept this kind of users engaged. Driven by previous traffic these recommendations were fuelled by other pedophiles. A problem that YouTube has faced since 2017, still ongoing with no end in sight. When this controversy came up, prominent advertisers were very clear that they would pull out of YouTube if it was not fixed soon. YouTube’s solution? They removed accounts of the pedophiles, demonetized videos, and disabled the comment sections. This is YouTube playing whack-a-mole without changing structural incentives. YouTuber Matt Stone who highlighted these practices in 2019 in his video describes this as an ‘algorithmic wormhole’ that can be found in mere seconds.

Another mole that got whacked are the clickbait titles, with harmful content that got recommended often by YouTube’s algorithms. In their official blog, the YouTube team shared their solution of recommending based on likes and dislikes rather than the number of views, in order to reduce the presence of clickbait. However, remembering the opportunity to buy fake subscribers, which can like fake news videos computer-based, this change in the algorithms is not worth a penny.

In their latest update, the YouTube team mentions the reduction in recommendations of harmful videos, that come close to the line of the Community Guidelines, such as videos claiming the earth is flat, or false claims on historic events like 9/11 or the holocaust. However, YouTube is only working on reducing the presence of these videos in the recommendations overall. This means the videos will still be available on the platform, and can still be reached when users are on track towards those subjects. In fact, the only change that happens is the length of the road towards extremist videos. Keeping in mind the comparison to a restaurant where only sugary fat meals are served over and over again, this measure will not have any effect in the long run.

What the stated examples have in common, is the symbolic nature. It seems that YouTube only takes symbolic measures to fix the noticeable symptoms, by doing just enough to spin the media narrative and to keep regulators happy, however, the company never touches the root cause of the problem.

## Proposed Scheme for Improvements
As YouTube operates in an electronic global environment that crosses borders, it is impossible to find a one-size-fits-all solution to the negative impact the recommender systems have on society. YouTube can be seen as a natural monopoly as a video platform, which means they hold all the power. Next to that, the many values involved, such as freedom of speech, democratic sovereignty and censorship, make it hard to keep all parties happy. So instead of searching for the policy that will fix the world, we propose a scheme of improvements to the system.

The first part of this scheme is to bring back old friends into the recommendation system in addition to YouTube’s recommendations. Making use of the friend system in YouTube, users can recommend videos to each other, which will be automatically added to the autoplaylist of the friends selected by the sharer. When all the videos recommended by a user’s friends are watched, the system returns to its current state of using YouTube’s recommended videos. As this function will be used more over time, the time spent on YouTube will exist for a bigger part of videos recommended by friends. A big impact this will make is the effect of social norms feeding into the recommendations.

The solution is, however, not perfect. Firstly, it is likely that there will be ‘influencers’ with an enormous amount of friends recommending videos. Secondly, it is very easy to get stuck in filter bubbles as well, only this time together with your friends. However, there is a shift of power from YouTube back to society. Whenever you get a video recommended you know it is from someone you are familiar with instead of a corporate entity. At the same time, the business model of YouTube does not have to change a single bit, but could even result in better user satisfaction.

The second part of the scheme incorporates society as a whole. Kevin Roose, a columnist for Business and a writer-at-large for The New York Times, talks of “deprogramming a generation” as a necessity to change the impact of platforms like YouTube. This part focuses on giving the people the tools to examine sources critically. YouTube is not just a place to find entertainment, but as was found by Pearson Education, over 50% of youngsters reaches out to YouTube as an information source. The platform should be seen as the library of the now. It is therefore important to inform the users which of the “books” can be seen as reliable, useful and trustworthy, just like it is taught in schools for books or internet websites. This collaborative effort between the government and YouTube should start in school but also on YouTube itself. This way the minds of the users will be changed, and harmful videos will be evaded. Specifically, an informative video could be created that explains how to interact with the different kinds of videos on YouTube.

The third and last part of our scheme that will shrink the negative impact of YouTube is to make the platform responsible for the videos they recommend. This would mean slightly pulling YouTube out of its facilitating role, causing them to be more involved with the content displayed on their platform. A legal framework should be created indicating the responsibility of YouTube and the changes the platform should make in order to recommend only the fruitful videos to users. This responsibility would at least remove the fake news videos that have been duplicated multiple times and viewed/liked by fake subscribers. If YouTube would be responsible for the videos they recommend, these types of fake, harming videos would not have a place in the auto playlists any longer. It is important to stress that this part of the scheme does not influence the freedom of speech value, as every legal video will stay available on the platform. No censorship takes place in what can be uploaded, recommendations will only become more thoughtful.

## Conclusion
The scheme sketched above can be seen as the way society started to deal with cigarettes not that long ago. Friends would not recommend cigarettes to each other, due to the social norms that exist. Of course smokers would recommend them, but this is inevitable. Likewise, extremist people are always going to find their way into extremist videos, and will recommend them to their friends as well. However, adding social norms to the system will decrease the overall use of the undesirable product. The education part of the scheme can also be seen in the use of cigarettes. Parents teach their children that smoking is bad, schools teach their students the same thing. By creating an appealing video that explains the dangers of extremist videos, but also shows how to interact with them, people will learn to evade such videos. Lastly, like society stopped the promotion of cigarettes, in the future the responsible YouTube will stop promoting (recommending) extremist videos completely. The scheme is a first step in regulating the influence of YouTube on our daily lives. When the walk is continued, YouTube will be a safe environment for relaxing distraction. It will not settle all the problems that YouTube’s algorithms inhabit, but at least we will be able to watch our daily chess videos peacefully.


## References
Bergen, M. (2019). YouTube Executives Ignored Warnings, Let Toxic Videos Run Rampant - Bloomberg. Retrieved April 18, 2019, from https://www.bloomberg.com/news/features/2019-04-02/youtube-executives-ignored-warnings-letting-toxic-videos-run-rampant

Chaslot, G. (2016, November 27). YouTube’s A.I. was divisive in the US presidential election. Retrieved March 26, 2019, from Medium website: https://medium.com/the-graph/youtubes-ai-is-neutral-towards-clicks-but-is-biased-towards-people-and-ideas-3a2f643dea9a

Covington, P., Adams, J., & Sargin, E. (2016, September). Deep neural networks for youtuberecommendations. In Proceedings of the 10th ACM conference on recommender systems (pp. 191-198). ACM.

Fagan, K. (2018, February 22). Twitter wants to shield survivors of the Parkland shooting from online harassment as conspiracy theories spread. Retrieved April 18, 2019, from Business Insider Nederland website: http://www.businessinsider.com/twitter-parkland-shooting-survivors-online-harassment-2018-2

Kitchin, R. (2014). Thinking Critically About and Researching Algorithms (SSRN Scholarly Paper No. ID 2515786). Retrieved from Social Science Research Network website: https://papers.ssrn.com/abstract=2515786

Lapowsky, I. (2018). YouTube Debuts Plan to Promote and Fund “Authoritative” News | WIRED. Retrieved March 26, 2019, from https://www.wired.com/story/youtube-debuts-plan-to-promote-fund-authoritative-news/

Matsakis, L. (2018). YouTube Will Link Directly to Wikipedia to Fight Conspiracy Theories | WIRED. Retrieved March 26, 2019, from https://www.wired.com/story/youtube-will-link-directly-to-wikipedia-to-fight-conspiracies/

Netwon, C., (2017). How YouTube perfected the feed., Retrieved March 25th 2019 from 
https://www.theverge.com/2017/8/30/16222850/youtube-google-brain-algorithm-video-rec
Ommendation-personalized-feed

Nicas, J. (2017, October 6). YouTube Tweaks Search Results as Las Vegas Conspiracy Theories Rise to Top. Wall Street Journal. Retrieved from https://www.wsj.com/articles/youtube-tweaks-its-search-results-after-rise-of-las-vegas-conspiracy-theories-1507219180

Orphanides, K. G. (2019, February 20). On YouTube, a network of paedophiles is hiding in plain sight. Wired UK. Retrieved from https://www.wired.co.uk/article/youtube-pedophile-videos-advertising

Pearson Education. (2017). Engage from A to Gen Z (p. 1).
Pentland, A. (2014). Social physics: How good ideas spread-the lessons from a new science. London: Penguin Books.

Suzor, N. (2019, March 20). YouTube nukes its API and search functionality in response to Christchurch massacre. Retrieved March 26, 2019, from Digital Social Contract website: https://digitalsocialcontract.net/youtube-nukes-its-api-and-search-functionality-in-response-to-christchurch-massacre-6051b4f2bb77

Tufekci, Z. (2018, June 8). Opinion | YouTube, the Great Radicalizer. The New York Times. Retrieved from https://www.nytimes.com/2018/03/10/opinion/sunday/youtube-politics-radical.html

YouTube. (n.d.). Continuing our work to improve recommendations on YouTube. Retrieved March 26, 2019, from Official YouTube Blog website: https://youtube.googleblog.com/2019/01/continuing-our-work-to-improve.html

Zuboff, S. (2016, May 3). Google as a Fortune Teller: The Secrets of Surveillance Capitalism. Retrieved from https://www.faz.net/1.4103616